{
 "metadata": {
  "name": "Logbook"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# ISFINDER NOTEBOOK\n",
      "\n",
      "# 2014 05 20\n",
      "\n",
      "### Input Data\n",
      "\n",
      "* Paired fastq.gz file (an artificial dataset can be generated using [ISIS](https://github.com/a-slide/Isis)\n",
      "* Reference genome of virus and host DNA in fasta format\n",
      "* Annotation files of host reference genome to locate features such as genes, CpG, active chromatine... (Bed format?)\n",
      "* Configuration file containing all algorithm parameters\n",
      "* output name use as a prefix for output data\n",
      "\n",
      "### Required Output (for biologists...)\n",
      "\n",
      "* After a de *novo* reconstruction of junctions from chimeric pairs and reads a report containing the characteristics of each junction and the number of reads in support of its exxistence will be generated\n",
      "\n",
      "* To visualize junctions a fasta reference of reconstructed will be generated and BAM/BAI/bedGraph file containing supported reads will also be generated \n",
      "\n",
      "* In addition, BAM, BAM index and BedGraph for the following categories of sequences\n",
      "    * Unmapped sequences (BAM only)\n",
      "    * Both mate of a pair in virus genome\n",
      "    * Both mate of a pair in host genome\n",
      "    * Pairs ovelapping true junctions\n",
      "    * Pairs corresponding to NGS prep artifacts (not a true junction)\n",
      "\n",
      "* A circos plot of insertion distribution in host genome will be created with Circos\n",
      "\n",
      "* Using host genome annotation feature, the enrichment of junction nears specific feature will be summurize in a report (CpG, Gene, heterochromatin...)\n",
      "\n",
      "### Specificity of the analysis\n",
      "\n",
      "1. Extract virus/host genome junctions in pair end sequencing data : Junction could be inside a read (chimeric read) or between the 2 mate of a pair (chimeric pairs)\n",
      "2. Bad quality sequences may result in unmapped reads or false positive\n",
      "3. A large number of sequence may result in extensive calculation time   \n",
      "4. Homology between virus and fragment of host genome may result in false positive junctions.\n",
      "5. Filter out false junctions created during NGS library preparation\n",
      "\n",
      "\n",
      "### Suggested solutions\n",
      "\n",
      "1. Map the dataset against a fused reference containg both virus and host genome with a BWT based aligner. The aligner need to be able to detect and report reads party aligned on 2 reference2. The SAM result can be subsequently pardes to extract chimeric reads. \n",
      "2. To avoid a negative impact of bad quality and unrelated sequence, a primary facultative step including quality and adapter trimming will be implemented\n",
      "3. If possible, heavy calculating steps will be parallelize with either CPU or GPU distribution. Bowtie2, BWA, BWA_PSSM and BALSA will be benchmarked side by side to compare precision and execution time with a artificial dataset generated with [ISIS](https://github.com/a-slide/Isis).\n",
      "4. A facultative step of homology masking in host genome will be implemented using results of a virus genome blast against the host genome. We will compared the precision with or without this step.\n",
      "5. From all chimeric reads and pair a *de novo* assembly to reconstruct junctions will performed. True junctions will be defined by selecting junctions supported by either a least a fixed count (ex = 5) or above a dynamic background established thanks to a biological control (non infected animal)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2014 05 21\n",
      "\n",
      "###\u00a0Initial design proposed for IsFinder\n",
      "\n",
      "![Design](files/img/Initial_design.svg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exploration of Blast integration in biopython = Drafting function for CoordHits\n",
      "\n",
      "NCBIWWW module = BLAST over the Internet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Bio.Blast.NCBIWWW import qblast\n",
      "qblast('blastn', 'nr', 'ATATCCGTAGTATTTATATGCGCGTAGCTCGCTGCTTATTATCGCTGATCTC')\n",
      "# Dos not work in the lab due to the proxy server..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In all cases this online requesting tool is not suitable for IsFind pipeline. Blast need to be run locally with a local dadabase consisting only of host genomic DNA. For this we can use the NCBI BLASTX wrapper from the Bio.Blast.Application"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Bio.Blast.Applications import NcbiblastxCommandline as blast\n",
      "help(blast)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A blast database had to be prepared before using blast\n",
      "\n",
      "Test of makeblast in bash shell.\n",
      "\n",
      "call or check call had to be used to launch command line in the shell\n",
      "check call is better since it raise a CalledProcessError in case of failure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# command line with an error\n",
      "from subprocess import check_call\n",
      "check_call((\"makeblastdb -in multi.fa -dbtype nucl -input_type fast\"), shell=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# command line without error\n",
      "check_call((\"makeblastdb -in multi.fa -dbtype nucl -input_type fasta\"), shell=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Command line execution function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from subprocess import check_call, CalledProcessError\n",
      "\n",
      "def command_bool(command):\n",
      "    \"\"\" Generic function to execute command lines return true or false\n",
      "    \"\"\"\n",
      "    try:\n",
      "        print (command)\n",
      "        check_call((command), shell=True)\n",
      "        print (\"Success\")\n",
      "    except (OSError, CalledProcessError) as E:\n",
      "        print (E)\n",
      "        exit (0)\n",
      "\n",
      "#Definition of the reference and construction of the command text line\n",
      "reference = \"multi.fa\"\n",
      "command = \"makeblastdb -in {0} -dbtype nucl -input_type fasta\".format(reference)\n",
      "\n",
      "# Call of command_line\n",
      "command_bool(command)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Blastn using biopython parser = not much informative and can be easily done without"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Bio.Blast.Applications import NcbiblastnCommandline as blastn\n",
      "\n",
      "command = blastn(query=\"frag.fa\", db=\"multi.fa\", evalue=0.001, outfmt=1, out=\"test.xml\")\n",
      "print (command)\n",
      "\n",
      "stdout, stderr = command()\n",
      "print (stdout)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using my command_bool functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = \"frag.fa\"\n",
      "db = \"multi.fa\"\n",
      "command = \"blastn -outfmt '6 sseqid sstart send' -query {0} -db {1} -evalue 0.001\".format(query, db)\n",
      "\n",
      "# Call of command_line\n",
      "print (command_bool(command))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Call or check_call are not able to return the command line return value. Instead Popen should be used for this"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from subprocess import Popen, PIPE\n",
      "\n",
      "def command_return(command):\n",
      "    \"\"\" Generic function to execute command lines return true or false\n",
      "    \"\"\"\n",
      "    try:\n",
      "        print (command)\n",
      "        return Popen(command, shell=True, stdout=PIPE).stdout.read()\n",
      "    except (OSError, ValueError) as E:\n",
      "        print (E)\n",
      "        exit (0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = \"frag.fa\"\n",
      "db = \"multi.fa\"\n",
      "command = \"blastn -outfmt '6 sseqid sstart send' -query {0} -db {1} -evalue 0.001\".format(query, db)\n",
      "\n",
      "# Call of command_line\n",
      "command_return(command)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It work and return a list of string with each string composed of the subject ref name followed by the start and end of the blast hit. Now Let's build fonctions to parse this results and store it in a dictionnary.\n",
      "\n",
      "# 2014 05 22\n",
      "\n",
      "### Split lines from a raw blast CSV output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_blast_lines (command):\n",
      "    return [i for i in command_return(command).splitlines()]\n",
      "        \n",
      "split_blast(\"blastn -outfmt '6 sseqid sstart send' -query frag.fa -db multi.fa -evalue 0.001\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### List Version of blast hits results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def list_blast (command):\n",
      "    return [i.split(\"\\t\") for i in split_blast_lines (command)]\n",
      "\n",
      "list_blast(\"blastn -outfmt '6 sseqid sstart send' -query frag.fa -db multi.fa -evalue 0.001\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Dict Version of blast hits results (with additional str to int conversion of start and end)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dict_blast (command):\n",
      "    d = {}\n",
      "    for ref, start, end in list_blast(command):\n",
      "        if ref in d:\n",
      "            d[ref].append([int(start), int(end)])\n",
      "        else:\n",
      "            d[ref] = [[int(start), int(end)]]\n",
      "    return d\n",
      "\n",
      "dict_blast(\"blastn -outfmt '6 sseqid sstart send' -query frag.fa -db multi.fa -evalue 0.001\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Summary of functions for CoordHits\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from subprocess import Popen, PIPE\n",
      "\n",
      "def command_return(command):\n",
      "    try:\n",
      "        return (Popen(command, shell=True, stdout=PIPE).stdout.read())\n",
      "    except (OSError, ValueError) as E:\n",
      "        print (E)\n",
      "        exit (0)    \n",
      "\n",
      "def split_blast_lines (command):\n",
      "    return [i for i in command_return(command).splitlines()]\n",
      "\n",
      "def list_blast (command):\n",
      "    list_blast = [i.split(\"\\t\") for i in split_blast_lines (command)]\n",
      "    print (\"{0} hit(s) found\".format(len(list_blast)))\n",
      "    return list_blast\n",
      "\n",
      "def dict_blast (command):\n",
      "    d = {}\n",
      "    for ref, pos1, pos2 in list_blast(command):\n",
      "        \n",
      "        # Cast positions in integers\n",
      "        pos1 = int(pos1)\n",
      "        pos2 = int(pos2)\n",
      "        \n",
      "        # Return end and start in the appropriate order\n",
      "        start = pos1 if pos1 <= pos2 else pos2\n",
      "        end = pos2 if pos1 <= pos2 else pos1\n",
      "        \n",
      "        # Fill the dictionary iteratively\n",
      "        if ref in d:\n",
      "            d[ref].append([start, end])\n",
      "        else:\n",
      "            d[ref] = [[start, end]]\n",
      "    return d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Definition of the reference and construction of the command text line\n",
      "query = \"../data/AAV_RSV_GFP.fa\"\n",
      "subject = \"../data/mm10.fa\"\n",
      "cmd = \"makeblastdb -in {0} -dbtype nucl -input_type fasta\".format(subject)\n",
      "\n",
      "print (\"Creating subject database\") \n",
      "print(command_return(cmd))\n",
      "\n",
      "print (\"Blast {0} against {1} database\".format(query, subject))\n",
      "cmd = \"blastn -outfmt '6 sseqid sstart send' -query {0} -db {1} -evalue 0.1\".format(query, subject)\n",
      "dict_blast(cmd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It works with the whole mouse genome versus an AAV genome. It is really fast even taking into account database building (> 1 min). However Blast option needs to be optimize since only few hits are found only in chr12. The blast User Manuel may help to determine better parameters [USER MANUAL](http://www.ncbi.nlm.nih.gov.gate2.inist.fr/books/NBK1763/?report=reader)\n",
      "\n",
      "blastn algorithm seems more adapted than the default option megablast to find partial matches, but it is also a little slower.\n",
      "Parameters were change to increase the number of hits:\n",
      "\n",
      "* -evalue was remove to reach its default value = 10\n",
      "* -dust was deactivated to avoid query filtering\n",
      "* -num_threads was added to use parralel CPU processing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from multiprocessing import cpu_count\n",
      "\n",
      "def blastn_hits (query, subject, evalue=10):\n",
      "    \n",
      "    cmd = \"makeblastdb -in {0} -dbtype nucl -input_type fasta\".format(subject)\n",
      "    print (\"Creating subject database\") \n",
      "    print(command_return(cmd))\n",
      "    \n",
      "    cmd = \"blastn -task blastn -outfmt '6 sseqid sstart send' -dust no -num_threads {0} -evalue {1} -query {2} -db {3}\".format(\n",
      "        cpu_count(),\n",
      "        evalue,\n",
      "        query,\n",
      "        subject)\n",
      "    \n",
      "    print (\"Blast query against subject database\")\n",
      "    return (dict_blast(cmd))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blastn_hits (\"../data/AAV_RSV_GFP.fa\", \"../data/mm10.fa\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These parameters sound suitable for a future implementation of HitCoord class"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using hit coordinates to clean host ref from Blast results Draft of Ref Curator class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Bio import SeqIO\n",
      "\n",
      "# FUNCTION FROM ISIS CLASS REFERENCEGENOME\n",
      "def import_fasta(filename):\n",
      "    \"\"\"Import fasta files in a dictionary of biopython SeqRecord\n",
      "    \"\"\"\n",
      "    # Try to open the file fist gz compressed and uncompressed\n",
      "    try: \n",
      "        if filename.rpartition(\".\")[-1] == \"gz\":\n",
      "            print(\"\\tUncompressing and extracting data\")\n",
      "            handle = gzip.open(filename, \"r\")\n",
      "        else:\n",
      "            print(\"\\tExtracting data\")\n",
      "            handle = open(filename, \"r\")\n",
      "\n",
      "        d = SeqIO.to_dict(SeqIO.parse(handle, \"fasta\"))\n",
      "        handle.close()\n",
      "        \n",
      "        return d\n",
      "        \n",
      "    # Try to open the file fist gz compressed and uncompressed\n",
      "    except IOError:\n",
      "           print('CRITICAL ERROR. The fasta file ' + filename + ' is not readable. Exit')\n",
      "           exit\n",
      "\n",
      "def curate_ref (query, subject, evalue):\n",
      "    \n",
      "    print (\"Searching for virus homologies in host genome\")\n",
      "    hits = blastn_hits(query, subject, evalue)\n",
      "    print (hits)\n",
      "    \n",
      "    if not hits:\n",
      "        print (\"No hits found. The original host genome sequence will not be edited\")\n",
      "    else :\n",
      "        print (\"\\nImporting host genome for hard masking of homologies\")\n",
      "        genome = import_fasta(subject)\n",
      "        \n",
      "        # For all reference in which a blast hit was found \n",
      "        for ref, coord_list in hits.items():\n",
      "            # Transform the reference sequence to modify in a mutable sequence\n",
      "            print (\"Editing reference sequence : {}\".format(ref))\n",
      "            genome[ref].seq = genome[ref].seq.tomutable()\n",
      "            # For all blast hit coord found in the current reference\n",
      "            for start, end in coord_list:\n",
      "                # For all position between start en end coordinates modify the base by N\n",
      "                for position in range (start, end):\n",
      "                    genome[ref].seq[position] = 'N'\n",
      "        \n",
      "        # Write the new reference in fasta format\n",
      "        genome_basename = subject.rpartition('/')[2].partition('.')[0]\n",
      "        print (\"Writing new version of {} in which homology with the virus are masked\".format(genome_basename))\n",
      "        with open(genome_basename + \"_masked.fa\", 'w') as f:\n",
      "            for ref in genome.values():\n",
      "                f.write(ref.format(\"fasta\"))\n",
      "    print (\"DONE\") \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "start_time = time.time()\n",
      "\n",
      "curate_ref(\"../data/frag.fa\", \"../data/multi.fa\", 10)\n",
      "\n",
      "print \"\\nExecution time {} seconds\".format(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It works pretty fast with a short sequence and subject. Now let's try it with the whole mouse genome vs an rAAV vector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "start_time = time.time()\n",
      "\n",
      "curate_ref(\"../data/AAV_RSV_GFP.fa\", \"../data/mm10.fa\", 10)\n",
      "\n",
      "print \"\\nExecution time {} seconds\".format(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That is really impressive taking into account that the mouse genome blast index was build, the mouse genome was imported, modified and rewritten..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2013 05 23\n",
      "\n",
      "Now that all function for Ref curator and Blastn are available and functional, I will transpose them in a object oriented model. There will be several modifications to the original organization.\n",
      "\n",
      "First blast hit will not been returned as a dictionnary of hits but as hit.objects tracked in a class list : Here is a preliminary code for this class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################################################################################\n",
      "\n",
      "class BlastHit(object):\n",
      "    \"\"\"Class description\n",
      "    \"\"\"\n",
      "####################################################################################################\n",
      "    \n",
      "####    CLASS FIELD    ####\n",
      "    \n",
      "    Instances = [] # Class field used for instance tracking \n",
      "    \n",
      "####    CLASS METHODS    ####\n",
      "    \n",
      "    @ classmethod\n",
      "    def count_total (self):\n",
      "        return (len(self.Instances))\n",
      "    \n",
      "    @ classmethod\n",
      "    def count_per_ref (self):\n",
      "        d = {}\n",
      "        for hit in Instances:\n",
      "            if hit.s_id in d:\n",
      "                d[ref] +=1\n",
      "            else:\n",
      "                d[ref] = 0\n",
      "        return d\n",
      "        \n",
      "    @ classmethod\n",
      "    def get (self):\n",
      "        return Instances\n",
      "        \n",
      "    @ classmethod\n",
      "    def getref (self, ref):\n",
      "        return [hit for hit in Instances if hit.s_id == \"ref\"]\n",
      "        \n",
      "####    FONDAMENTAL METHODS     ####\n",
      "   \n",
      "    def __init__(self, q_id,s_id, identity, length, mis, gap, q_start, q_end, s_start, s_end, evalue, bscore):\n",
      "        \"\"\"Object constructor\"\"\"\n",
      "        \n",
      "        # Name of the reference seq in the query and in the subject were a hit was found\n",
      "        self.q_id = q_id\n",
      "        self.s_id = s_id\n",
      "        # Percentage of identity\n",
      "        self.identity = float(identity)\n",
      "        # Length of the alignement\n",
      "        self.length = int(length)\n",
      "        # Number of mismatches and gaps\n",
      "        self.mis = int(mis)\n",
      "        self.gap = int(gap)\n",
      "        # Start and end position along the query sequence\n",
      "        self.q_orient = int(q_start) < int(q_end) # True if orientation is positive\n",
      "        self.q_start = int(q_start) if self.q_orient else int(q_end)\n",
      "        self.q_end = int(q_end) if self.q_orient else int(q_start\n",
      "        # Start and end position along the subject sequence\n",
      "        self.s_orient = int(s_start) < int(s_end) # True if orientation is positive\n",
      "        self.s_start = int(s_start) if self.s_orient else int(s_end)\n",
      "        self.s_end = int(s_end) if self.s_orient else int(s_start\n",
      "        # Evalue and bit score of the match\n",
      "        self.evalue = float(evalue)\n",
      "        self.bscore = int(bscore)\n",
      "        # Add the instance to the class instance tracking list\n",
      "        self.Instances.append(self)\n",
      "    \n",
      "    def __repr__(self):\n",
      "        \"\"\"Long representation\"\"\"\n",
      "        msg = \"{}\\n\".format(self.__str__())\n",
      "        msg += \"\\tQuery\\t{}:{}-{}({})\\n\".format(self.q_id, self.q_start, self.s_end, self.q_orient)\n",
      "        msg += \"\\tSubject\\t{}:{}-{}({})\\n\".format(self.q_id, self.q_start, self.s_end, self.q_orient)\n",
      "        msg += \"\\tLenght : {}\\tIdentity : {}%\\tEvalue : {}\\tBit score : {}\\t\".format(self.length, self.identity, self.evalue, self.bscore)\n",
      "        \n",
      "    def __str__(self):\n",
      "        \"\"\"Short representation\"\"\"\n",
      "        return \"<Instance of \" + self.__module__ + \">\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2014 05 26\n",
      "\n",
      "## @ Blast Module\n",
      "\n",
      "The entire Module is working properly. It consists of 2 classes :\n",
      "\n",
      "* Blast is a singleton class that contains functions to execute makeblastdb (blast database creation) and blastn.\n",
      "* BlastHit is instancied for each blast hit found by do_blast() from Blast class (see above).\n",
      "\n",
      "in order to reuse generic utilities in future classes, I created a utilities module in which I put run_shell() and mkdir() functions. More function should be added during the developpement.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2014 05 27\n",
      "\n",
      "## General\n",
      "Introduction of doxygen documentation synthax to allow a simple and complete auto-generation of the documentation [Doxygen](http://www.stack.nl/~dimitri/doxygen/index.html)\n",
      "\n",
      "Doxygen can be easily integrated in python docstring using [doxypy](http://code.foosel.org/doxypy).\n",
      "\n",
      "The synthax is really simple and text bloc can be formated using markdown :\n",
      "\n",
      "* @bref : Brief description\n",
      "* @author : List of authors\n",
      "* @param : Description of each parameters\n",
      "* @return : Description of the return value\n",
      "* @exception : Description of possible exceptions raised by the function\n",
      "* ...\n",
      "\n",
      "Complete implementation of doxigen documentation in all functions, classes and package.\n",
      "\n",
      "Restructuration of the initial classes = grouping Blast, BlastHit and RefCurator (now called RefMasker) in a single package (same file) = HomologyMasker\n",
      "A masker() function was writen in RefMasker in order to be able to blast multiple query references against the reference database in a row. This function was not really critical for Isfinder, but it is necessary for the future analysis pipeline another of my NGS project.\n",
      "\n",
      "See an exemple with a small dataset bellow "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from HomologyMasker import RefMasker\n",
      "RefMasker.masker([\"../test/q1.fa\", \"../test/q2.fa\", \"../test/q3.fa\"], \"../test/s1.fa\", 0.0000001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### TODO\n",
      "* Find a strategy to trim adapters and quality\n",
      "* Start drafting of fastqTrimmer and fastqAligner"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2014 06 16\n",
      "\n",
      "## Draft fastq import and qualFilter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Bio import SeqIO\n",
      "import gzip\n",
      "\n",
      "print(\"Uncompressing and extracting data\")\n",
      "R1 = gzip.open(\"../test/fastq/small.R1.fastq.gz\", \"r\")\n",
      "R2 = gzip.open(\"../test/fastq/small.R2.fastq.gz\", \"r\")\n",
      "\n",
      "genR1 = SeqIO.parse(R1, \"fastq\")\n",
      "genR2 = SeqIO.parse(R2, \"fastq\")\n",
      "\n",
      "ListR1 = []\n",
      "ListR2 = []\n",
      "\n",
      "while True:\n",
      "    seqR1 = next(genR1, None)\n",
      "    seqR2 = next(genR2, None)\n",
      "    \n",
      "    if not seqR1 or not seqR2:\n",
      "        print (\"End of list\")\n",
      "        break\n",
      "    \n",
      "    # Compute mean quality score\n",
      "    qMeanR1 = float(sum(seqR1.letter_annotations['phred_quality']))/len(seqR1)\n",
      "    qMeanR2 = float(sum(seqR2.letter_annotations['phred_quality']))/len(seqR2)\n",
      "    # Filter read pair based on the mean quality\n",
      "    if qMeanR1 >= 32 and qMeanR2 >= 32:\n",
      "        ListR1.append(seqR1)\n",
      "        ListR2.append(seqR2)\n",
      "        \n",
      "print (len(ListR1))\n",
      "\n",
      "R1.close()\n",
      "R2.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Uncompressing and extracting data\n",
        "End of list\n",
        "16\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Fastq import and quality score is working properly**\n",
      "\n",
      "## Draft primer trimmer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
      "\n",
      "def find_matches(sequence, primer_list):\n",
      "    \n",
      "    match_list = []\n",
      "\n",
      "    for primer in primer_list:\n",
      "        match_gen = re.finditer(primer, sequence, re.I)\n",
      "        for match in match_gen:\n",
      "            match_list.append([match.start(), match.end()])\n",
      "    return (match_list)\n",
      "\n",
      "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
      "\n",
      "def longer_interval(match_list, maxsize):\n",
      "    \n",
      "    start_max = 0\n",
      "    end_max = 0\n",
      "    inter_max = 0\n",
      "    end_prec = -1\n",
      "    \n",
      "    match_list.sort()\n",
      "    match_list.append([maxsize, 0])\n",
      "    print (match_list)\n",
      "    \n",
      "    for start, end in match_list:\n",
      "        if end > end_prec:\n",
      "            if start - end_prec - 1 > inter_max:\n",
      "                start_max = end_prec\n",
      "                end_max = start\n",
      "                inter_max = start - end_prec - 1\n",
      "            end_prec = end \n",
      "    print (\"Longer interval = {} [{}:{}]\".format(inter_max, start_max+1, end_max-1))\n",
      "    return (start_max+1, end_max-1)\n",
      " \n",
      "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
      "\n",
      "PRIMER_LIST = [\"gcgcg\", \"CGcC\"]\n",
      "MIN_SIZE = 50\n",
      "\n",
      "for mate, fastq_list  in [[\"R1\", ListR1], [\"R2\", ListR2]]:\n",
      "    print (\"\\nORIENTATION \" + mate + \"\\n\")\n",
      "  \n",
      "    for i in range(len(fastq_list)):\n",
      "        print i\n",
      "        match_list = find_matches(str(fastq_list[i].seq), PRIMER_LIST)\n",
      "        \n",
      "        if match_list:\n",
      "            print (\"{} hit(s) found\".format(len(match_list)))\n",
      "            start, end = longer_interval (match_list, len(fastq_list[i])) \n",
      "            if end - start < MIN_SIZE:\n",
      "                print (\"Trimming resulted in short sequence. Pair removed\") \n",
      "                ListR1.pop(i)\n",
      "                ListR2.pop(i)\n",
      "            else:\n",
      "                fastq_list[i] = fastq_list[i][start:end]\n",
      "        \n",
      "    for fastq in ListR1:\n",
      "        print (\"{} : {}\\n\".format(fastq.id, fastq.seq))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "list index out of range",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-40-4f868981f8b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfastq_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmatch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfastq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRIMER_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatch_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: list index out of range"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ORIENTATION R1\n",
        "\n",
        "0\n",
        "1 hit(s) found\n",
        "[[145, 150], [150, 0]]\n",
        "Longer interval = 145 [0:144]\n",
        "1\n",
        "1 hit(s) found\n",
        "[[128, 133], [150, 0]]\n",
        "Longer interval = 128 [0:127]\n",
        "2\n",
        "1 hit(s) found\n",
        "[[114, 119], [150, 0]]\n",
        "Longer interval = 114 [0:113]\n",
        "3\n",
        "4\n",
        "3 hit(s) found\n",
        "[[1, 6], [62, 67], [113, 117], [150, 0]]\n",
        "Longer interval = 55 [7:61]\n",
        "5\n",
        "6\n",
        "7\n",
        "1 hit(s) found\n",
        "[[92, 97], [150, 0]]\n",
        "Longer interval = 92 [0:91]\n",
        "8\n",
        "3 hit(s) found\n",
        "[[52, 57], [103, 107], [143, 147], [150, 0]]\n",
        "Longer interval = 52 [0:51]\n",
        "9\n",
        "10\n",
        "3 hit(s) found\n",
        "[[2, 6], [42, 46], [100, 105], [150, 0]]\n",
        "Longer interval = 53 [47:99]\n",
        "11\n",
        "3 hit(s) found\n",
        "[[63, 67], [102, 106], [138, 142], [150, 0]]\n",
        "Longer interval = 63 [0:62]\n",
        "12\n",
        "1 hit(s) found\n",
        "[[79, 84], [150, 0]]\n",
        "Longer interval = 79 [0:78]\n",
        "13\n",
        "1 hit(s) found\n",
        "[[73, 77], [150, 0]]\n",
        "Longer interval = 73 [0:72]\n",
        "14\n",
        "1 hit(s) found\n",
        "[[9, 13], [150, 0]]\n",
        "Longer interval = 9 [0:8]\n",
        "Trimming resulted in short sequence. Pair removed\n",
        "15\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2014 06 17\n",
      "\n",
      "After investigation the way adapter cutting is usually done in other NGS quality/primer trimmer is done, It seems that it is usually more complicated than the way used in my draft. Imperfect matches are often taken into account as well as quality string and eventual pair overlap informations.\n",
      "There are many existing tools that can do this job pretty well. I decided to start by using skewer a new tools recently published descibed to outperformed its concurents [SKEWER](http://www.biomedcentral.com/1471-2105/15/182/abstract). I may be interesting to try cutadapt after since it is a python script, but unfortunalty it isn't multithreaded and not developped for pair end data [CUTADAPT](https://code.google.com/p/cutadapt/).\n",
      "The problem with all these independant programs is that I will be forced to write intermediate files on disk (as opposed to stored in RAM). A possibility would be to use temp files [Tempfile module](https://docs.python.org/2/library/tempfile.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Utilities import run_command"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_command(\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}